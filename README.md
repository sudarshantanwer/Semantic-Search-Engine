# ğŸ“š Semantic Search Chatbot

A full-stack AI-powered chatbot that performs semantic search using OpenAI embeddings and displays results via a React frontend. The backend uses FastAPI and LangChain with Chroma vector DB. This app is both development and production-ready, with support for Docker and deployment to cloud platforms like AWS.

---

## ğŸ”§ Project Structure

semantic-search-chatbot/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ main.py # FastAPI app
â”‚ â”œâ”€â”€ data/document.txt # Source text file
â”‚ â”œâ”€â”€ chroma_db/ # Vector DB will be persisted here
â”‚ â”œâ”€â”€ .env # Contains OPENAI_API_KEY
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ components/ChatBox.jsx
â”‚ â”‚ â””â”€â”€ main.jsx
â”‚ â”œâ”€â”€ index.html
â”‚ â””â”€â”€ package.json
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md


---

## ğŸš€ Deployment Options

This app can be run in two ways:

- âœ… Local Dev Setup (with virtualenv + Yarn)
- ğŸ³ Docker-based Setup (recommended for production)

---

## ğŸ§ª Local Setup (Dev Mode)

### âš™ï¸ Backend (FastAPI + LangChain)

1. **Navigate to backend and create virtual environment:**

```bash
cd backend
python -m venv venv
source venv/bin/activate   # macOS/Linux
venv\Scripts\activate      # Windows
pip install -r requirements.txt

# Add your .env file in backend with these keys/:
OPENAI_API_KEY=your_openai_api_key

# Start the backend server:
uvicorn main:app --reload

# Runs at: http://localhost:8000



## Frontend (React + Yarn)
# Navigate to frontend and install dependencies:
cd ../frontend
yarn install

# Start the React dev server:
yarn dev
# Runs at: http://localhost:5173




# ğŸ³ Docker Setup (Production-Ready)
# Recommended for AWS EC2 or any cloud server.

# ğŸ“‹ Prerequisites
# Docker installed: https://docs.docker.com/get-docker/

# .env file added inside backend/ directory:

OPENAI_API_KEY=your_openai_api_key


# â–¶ï¸ Run with Docker Compose
docker-compose up --build

# This builds and starts:

# Backend at http://localhost:8000

# Frontend at http://localhost:5173

# ğŸ” Restart / Stop Containers
docker-compose restart        # restart all services
docker-compose down           # stop and remove services


# ğŸ” How It Works
# The backend loads data/document.txt and creates embeddings using OpenAI API.

# User submits a query via the React-based frontend.

# The backend finds the most semantically similar chunk using LangChain + Chroma.

# The best-matching response is returned and displayed.


# ğŸ”’ .gitignore Best Practices
# Ensure these are ignored:
frontend/node_modules
backend/venv
__pycache__/
.env




âœ… Tech Stack
Frontend: React (Vite), Yarn, Axios

Backend: FastAPI, LangChain, ChromaDB, OpenAI

Dev Tools: Docker, Docker Compose

Cloud Ready: Yes (Recommended for AWS/GCP/Azure)

